{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0109,  0.3142,  0.3060, -0.0961, -0.2895,  0.2136, -0.1708,\n",
      "          -0.0997,  0.0912, -0.0031, -0.1519,  0.0259,  0.0421,  0.1709,\n",
      "           0.2150, -0.1000,  0.3393,  0.2651, -0.0085, -0.0135,  0.0730,\n",
      "           0.0116,  0.1105, -0.1352, -0.0823,  0.3899,  0.1814,  0.1009,\n",
      "          -0.2097, -0.3307,  0.1143,  0.2149]],\n",
      "\n",
      "        [[ 0.0072,  0.3131,  0.3063, -0.0994, -0.2912,  0.2116, -0.1724,\n",
      "          -0.1033,  0.0965,  0.0024, -0.1512,  0.0233,  0.0406,  0.1782,\n",
      "           0.2066, -0.0972,  0.3377,  0.2652, -0.0051, -0.0142,  0.0742,\n",
      "           0.0048,  0.1180, -0.1363, -0.0826,  0.3924,  0.1792,  0.0996,\n",
      "          -0.2099, -0.3327,  0.1132,  0.2122]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "multihead_attn = nn.MultiheadAttention(32, 8)\n",
    "query = np.random.rand(2, 1, 32)\n",
    "key = np.random.rand(2, 1, 32)\n",
    "value = np.random.rand(2, 1, 32)\n",
    "attn_output, attn_output_weights = multihead_attn(Tensor(query), Tensor(key), Tensor(value))\n",
    "print(attn_output)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 32)\n",
      "[[[ 0.21025246 -1.3815589   1.1856667   0.5420509  -0.03878744\n",
      "    0.36193654 -0.30533046 -1.1700163  -0.57054275  0.46141142\n",
      "   -0.30795845  0.39367586 -0.54798263  0.19380882 -0.5910839\n",
      "   -0.4666907  -0.97481775  0.45212376 -0.10193141 -0.6575114\n",
      "    0.25120044  0.122086   -1.1403954  -0.42054623 -0.61952204\n",
      "   -0.6566897   0.73287785 -0.35822588 -0.5958375  -0.32572877\n",
      "    1.2399867  -0.22128716]]\n",
      "\n",
      " [[ 0.03517252 -0.6995417   0.97694     0.07682744  0.24728103\n",
      "    0.08959051  0.01169628 -0.8235637  -0.2518189   0.59149474\n",
      "   -0.40409014 -0.5977189   0.09025317  0.08469346 -0.29417813\n",
      "   -0.34428942 -0.61110383  0.7962756   0.02414452 -0.67949474\n",
      "    0.6065721  -0.24253327 -0.3226804   0.27078897 -1.3271445\n",
      "    0.25307137  0.3808884   0.18227924 -0.38555723 -0.42521483\n",
      "    1.2446275  -0.62895083]]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import flax.linen as nn\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "input = jnp.ones((2,1,32))\n",
    "layer = nn.MultiHeadAttention(num_heads=8, qkv_features=32)\n",
    "variables = layer.init(random.key(42), input)\n",
    "output = layer.apply(variables, Tensor(query), Tensor(key), Tensor(value))\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00069493 -0.03911137 -0.17373352 -0.08817849  0.00585917\n",
      "    0.0038011   0.13423112  0.12640306 -0.13031635 -0.10004204\n",
      "   -0.22439755 -0.07857274 -0.31675625 -0.07869545  0.18808702\n",
      "    0.18783928  0.21366805  0.16640219 -0.2740562   0.02806476\n",
      "   -0.16295458  0.29520345  0.17730789  0.13442674  0.35357815\n",
      "    0.15538093  0.08111625 -0.09462467 -0.03054206  0.1470299\n",
      "    0.03899852  0.20147526]]\n",
      "\n",
      " [[-0.00058219 -0.03654152 -0.17798103 -0.08727801  0.00280233\n",
      "    0.00116073  0.1349529   0.12789461 -0.13350655 -0.10066106\n",
      "   -0.2256635  -0.07729645 -0.31817535 -0.0761395   0.19332808\n",
      "    0.19098276  0.21913132  0.16640013 -0.27903354  0.03269039\n",
      "   -0.16899411  0.2947319   0.18080507  0.13609359  0.34764683\n",
      "    0.16034965  0.08051767 -0.09571798 -0.03570132  0.15065208\n",
      "    0.03331698  0.19594805]]]\n",
      "(2, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import nn\n",
    "from mindspore import Tensor\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(32, 8)\n",
    "attn_output, attn_output_weights = multihead_attn(Tensor(query, ms.float32), Tensor(key, ms.float32), Tensor(value, ms.float32))\n",
    "print(attn_output)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MindSpore MHA用法与输出与PyTorch一致**\n",
    "- MindSpore的Tensor()没有像torch.Tensor()一样内置类型转换，传入float64会报错"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
